<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Mutual Reinforcement Effect">
  <meta name="keywords" content="Information Extraction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mutual Reinforcement Effect</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://MRE.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://link.springer.com/chapter/10.1007/978-3-031-35320-8_18">SLGFramework</a>
          <a class="navbar-item" href="https://arxiv.org/abs/2309.03787">USAModel</a>
          <a class="navbar-item" href="https://arxiv.org/abs/2311.06838">GIELLM</a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models</h1>
<div class="is-size-5 publication-authors">
  <span class="author-block">
    <a href="ganchengguang.github.io">Chengguang Gan</a><sup>1</sup>,
  </span>
  <span class="author-block">
    <a href="#">Qingyu Yin</a><sup>2</sup>,
  </span>
  <span class="author-block">
    <a href="#">Xinyang He</a><sup>3,4</sup>,
  </span>
  <span class="author-block">
    <a href="#">Hanjun Wei</a><sup>4</sup>,
  </span>
  <span class="author-block">
    <a href="#">Yunhao Liang</a><sup>3</sup>,
  </span>
  <span class="author-block">
    <a href="#">Younghun Lim</a><sup>1</sup>,
  </span>
  <span class="author-block">
    <a href="#">Shijian Wang</a><sup>5</sup>,
  </span>
  <span class="author-block">
    <a href="#">Hexiang Huang</a><sup>6</sup>,
  </span>
  <span class="author-block">
    <a href="#">Qinghao Zhang</a><sup>7</sup>,
  </span>
  <span class="author-block">
    <a href="#">Shiwen Ni</a><sup>8</sup>,
  </span>
  <span class="author-block">
    <a href="#">Tatsunori Mori</a><sup>1</sup>
  </span>
</div>

<div class="is-size-5 publication-authors">
  <span class="author-block"><sup>1</sup>Yokohama National University,</span>
  <span class="author-block"><sup>2</sup>Zhejiang University,</span>
  <span class="author-block"><sup>3</sup>University of Chinese Academy of Sciences,</span>
  <span class="author-block"><sup>4</sup>Chengdu Institute of Computer Applications, Chinese Academy of Sciences,</span>
  <span class="author-block"><sup>5</sup>Southeast University,</span>
  <span class="author-block"><sup>6</sup>University of Tsukuba,</span>
  <span class="author-block"><sup>7</sup>Pusan National University,</span>
  <span class="author-block"><sup>8</sup>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences</span>
</div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.10953"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.10953"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ganchengguang/MRE.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ganchengguang/MMM-dataset-Trainset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> 
              <!-- Model Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/ganchengguang/OIELLM-8B-Instruction"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Model</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The Mutual Reinforcement Effect (MRE) represents a promising avenue in information extraction and multitasking research. 
            Nevertheless, its applicability has been constrained due to the exclusive availability of MRE mix datasets in Japanese, 
            thereby limiting comprehensive exploration by the global research community. To address this limitation, we introduce a 
            Multilingual MRE mix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and Chinese.
          </p>
          <p>
            In this paper, we also propose a method for dataset translation assisted by Large Language Models (LLMs), which 
            significantly reduces the manual annotation time required for dataset construction by leveraging LLMs to translate 
            the original Japanese datasets.
          </p>
          <p>
            Additionally, we have enriched the dataset by incorporating open-domain Named Entity Recognition (NER) and sentence 
            classification tasks. Utilizing this expanded dataset, we developed a unified input-output framework to train an 
            Open-domain Information Extraction Large Language Model (OIELLM). The OIELLM model demonstrates the capability to 
            effectively process novel MMM datasets, exhibiting significant improvements in performance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- <div class="columns is-centered"> -->



      <!-- MRE Figure -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Mutual Reinforcement Effect</h2>
          <p>
            The Mutual Reinforcement Effect between the labels of Word-level labels and text-level label within a same text.
          </p>
          <embed src="./static/1MRE.pdf" type="application/pdf" width="80%" height="500px" />
        </div>
      </div>
      
      <!--/ MRE Figure -->
      
      <!-- OIELLM Figure -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">OIELLM</h2>
          <p>
            The input and output of Open-domain Information Extraction Large Language Model (OIELLM).
          </p>
          <embed src="./static/5OIELLM.pdf" type="application/pdf" width="80%" height="500px" />
        </div>
      </div>
      <!--/ OIELLM Figure -->

      <!-- MMM Figure -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">MMM Datasets</h2>
          <p>
            Multilingual Mutual Reinforcement Effect Mix Datasets Names of all sub-datasets. 
            (The image does not represent a percentage of the actual subdataset size.)
          </p>
          <embed src="./static/2MMM.pdf" type="application/pdf" width="80%" height="500px" />
        </div>
      </div>
      <!--/ MMM Figure -->


    <!-- Concurrent Work. -->
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Related Links</h2>

    <div class="content has-text-justified">
      <p>
        Let me conclude by thanking the contributors to the MMM dataset for contributing the fundamental dataset. And the pioneering researchers who selflessly contributed.
      </p>
      <p>
        1. Japanese Wikipedia NER dataset - Takahiro Omi - <a href="https://github.com/stockmarkteam/ner-wikipedia-dataset">https://github.com/stockmarkteam/ner-wikipedia-dataset</a>
      </p>
      <p>
        2. JGLUE: Japanese General Language Understanding Evaluation - Kentaro Kurihara, Daisuke Kawahara, Tomohide Shibata - <a href="https://github.com/yahoojapan/JGLUE?tab=readme-ov-file">https://github.com/yahoojapan/JGLUE?tab=readme-ov-file</a>
      </p>
      <p>
        3. livedoor news corpus - 関口宏司 - <a href="https://www.rondhuit.com/download.html">https://www.rondhuit.com/download.html</a>
      </p>
      <p>
        4. UniversalNER - Wenxuan Zhou - <a href="https://arxiv.org/abs/2308.03279">https://arxiv.org/abs/2308.03279</a>
      </p>

      <p>
        Here is also some previous work on the MRE series. You may be able to get more information about MRE from these works.
      </p>
      <p>
        1. <a href="https://arxiv.org/abs/2307.10291">Mutual Reinforcement Effects in Japanese Sentence Classification and Named Entity Recognition Tasks</a> (2023) Chengguang Gan, Qinghao Zhang, Tatsunori Mori
      </p>
      <p>
        2. <a href="https://arxiv.org/abs/2309.03787">USA: Universal Sentiment Analysis Model & Construction of Japanese Sentiment Text Classification and Part of Speech Dataset</a> (2023) Chengguang Gan, Qinghao Zhang, Tatsunori Mori
      </p>
      <p>
        3. <a href="https://arxiv.org/abs/2311.06838">GIELLM: Japanese General Information Extraction Large Language Model Utilizing Mutual Reinforcement Effect</a> (2023) Chengguang Gan, Qinghao Zhang, Tatsunori Mori
      </p>
    </div>
  </div>
</div>

      
    </div>
  </div>
</div>

    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{gan2024mmmmultilingualmutualreinforcement, title={MMM: Multilingual Mutual Reinforcement Effect Mix
      Datasets & Test with Open-domain Information Extraction Large Language Models}, author={Chengguang Gan and Qingyu 
      Yin and Xinyang He and Hanjun Wei and Yunhao Liang and Younghun Lim and Shijian Wang and Hexiang Huang and
      Qinghao Zhang and Shiwen Ni and Tatsunori Mori}, year={2024}, eprint={2407.10953}, archivePrefix={arXiv},
      primaryClass={cs.CL}, url={https://arxiv.org/abs/2407.10953}, 
      }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
